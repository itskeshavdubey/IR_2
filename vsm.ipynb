{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Specify the folder path in your Google Drive where your corpus files are located\n",
    "corpus_folder = '/content/drive/My Drive/corpus_folder/'  # Replace with your folder name\n",
    "\n",
    "# Dynamically fetch all .txt files from the specified folder\n",
    "def get_corpus_files(folder_path):\n",
    "    corpus_files = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Only include .txt files\n",
    "            corpus_files.append(filename)\n",
    "    return corpus_files\n",
    "\n",
    "# Load and process corpus from Google Drive\n",
    "def load_corpus(corpus_files):\n",
    "    corpus = {}\n",
    "    for filename in corpus_files:\n",
    "        file_path = os.path.join(corpus_folder, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            corpus[filename] = file.read().lower().split()  # Tokenize by splitting on spaces\n",
    "    return corpus\n",
    "\n",
    "# Build index (dictionary and postings lists)\n",
    "def build_index(corpus):\n",
    "    dictionary = defaultdict(lambda: {'df': 0, 'postings': []})\n",
    "    document_lengths = {}\n",
    "    N = len(corpus)  # Total number of documents\n",
    "\n",
    "    # Build dictionary and postings\n",
    "    for docID, content in corpus.items():\n",
    "        term_freqs = defaultdict(int)\n",
    "        for term in content:\n",
    "            term_freqs[term] += 1\n",
    "        \n",
    "        # Update dictionary with term frequencies\n",
    "        for term, freq in term_freqs.items():\n",
    "            dictionary[term]['df'] += 1\n",
    "            dictionary[term]['postings'].append((docID, freq))\n",
    "        \n",
    "        # Calculate document length for normalization\n",
    "        doc_length = 0\n",
    "        for term, freq in term_freqs.items():\n",
    "            doc_length += (1 + math.log10(freq)) ** 2\n",
    "        document_lengths[docID] = math.sqrt(doc_length)\n",
    "    \n",
    "    return dictionary, document_lengths, N\n",
    "\n",
    "# Compute the tf-idf for query and documents\n",
    "def compute_tfidf(term, freq, df, N, for_query=False):\n",
    "    tf = 1 + math.log10(freq)\n",
    "    if for_query:\n",
    "        idf = math.log10(N / df)\n",
    "        return tf * idf\n",
    "    return tf  # For documents, we ignore the idf part in the lnc scheme\n",
    "\n",
    "# Rank documents based on cosine similarity\n",
    "def rank_documents(query, dictionary, document_lengths, N):\n",
    "    query_terms = query.lower().split()\n",
    "    query_freqs = defaultdict(int)\n",
    "    for term in query_terms:\n",
    "        query_freqs[term] += 1\n",
    "    \n",
    "    # Build the query vector\n",
    "    query_vector = {}\n",
    "    for term, freq in query_freqs.items():\n",
    "        if term in dictionary:\n",
    "            query_vector[term] = compute_tfidf(term, freq, dictionary[term]['df'], N, for_query=True)\n",
    "    \n",
    "    # Calculate cosine similarity for each document\n",
    "    scores = defaultdict(float)\n",
    "    for term in query_vector:\n",
    "        if term in dictionary:\n",
    "            postings = dictionary[term]['postings']\n",
    "            for docID, doc_freq in postings:\n",
    "                doc_tfidf = compute_tfidf(term, doc_freq, dictionary[term]['df'], N, for_query=False)\n",
    "                scores[docID] += query_vector[term] * doc_tfidf\n",
    "    \n",
    "    # Normalize scores by document lengths\n",
    "    for docID in scores:\n",
    "        scores[docID] /= document_lengths[docID]\n",
    "    \n",
    "    # Sort documents by score (highest first), then by docID in case of ties\n",
    "    ranked_docs = sorted(scores.items(), key=lambda x: (-x[1], x[0]))\n",
    "    return ranked_docs[:10]  # Return top 10 relevant documents\n",
    "\n",
    "# Display the results in a vertical list format\n",
    "def display_results(results):\n",
    "    print(\"Top 10Relevant Documents:\")\n",
    "    for rank, (doc, score) in enumerate(results, start=1):\n",
    "        print(f\"{rank}. {doc} (Relevance Score: {round(score, 5)})\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Step 3: Dynamically get all .txt files from the Google Drive folder\n",
    "    corpus_files = get_corpus_files(corpus_folder)\n",
    "    corpus = load_corpus(corpus_files)\n",
    "    dictionary, document_lengths, N = build_index(corpus)\n",
    "    \n",
    "    # User input query\n",
    "    query = input(\"Enter your search query: \")  # Accept query from user\n",
    "    ranked_docs = rank_documents(query, dictionary, document_lengths, N)\n",
    "    \n",
    "    # Display results in vertical list format\n",
    "    display_results(ranked_docs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
